# log_dir: output/log/pretrain_LibriTTS.2
# result_dir: output/result/dev.2

seed_everything: 42
model:
  class_path: projects.prune.systems.distill.DistillSystem
  init_args:
    preprocess_config: &preprocess_config config/preprocess/LibriTTS_VCTK.orig.yaml
    model_config: config/model/base.yaml
    train_config: &train_config
      - config/train/LibriTTS.yaml
      - config/train/base.yaml
    # algorithm_config: &algorithm_config config/algorithm/pretrain/dev.yaml
    algorithm_config: &algorithm_config config/algorithm/pretrain/pretrain_LibriTTS.2.yaml
    ckpt_path: output/ckpt/LibriTTS/meta-tts-accent-dev/1d34914e07244f87ad10c580ab85656c/checkpoints/epoch=39-step=39999.ckpt

data:
  class_path: lightning.datamodules.baseline_datamodule.BaselineDataModule
  init_args:
    preprocess_config: *preprocess_config
    train_config: *train_config
    algorithm_config: *algorithm_config
    stage: train

trainer:
  ### Train settings
  max_steps: 100000
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1

  ### Logging and saving settings
  enable_checkpointing: True
  default_root_dir: output/distill/baseline
  log_every_n_steps: 100
  check_val_every_n_epoch: 10

  ### GPU-related settings
  accelerator: gpu
  devices: [0]
  # strategy: ddp
  auto_select_gpus: True

  ### Callbacks
  # callbacks:
  #   - class_path: pytorch_lightning.callbacks.DeviceStatsMonitor
  #     init_args: {}
  #   - class_path: pytorch_lightning.callbacks.ModelCheckpoint
  #     init_args:
  #       save_top_k: -1
  #       every_n_epochs: 100
  #   #     save_weights_only: True
  #   - class_path: pytorch_lightning.callbacks.LearningRateMonitor
  #     init_args:
  #       log_momentum: True

  ### Malicious settings
  profiler: simple

  ### Depreciated settings
  # deterministic: True
