# log_dir: output/log/pretrain_LibriTTS.2
# result_dir: output/result/dev.2

seed_everything: 42
model: cli_config/prune_v1/post_maml/maml.yaml

data:
  class_path: lightning.datamodules.meta_datamodule.MetaDataModule
  init_args:
    preprocess_config: &preprocess_config config/preprocess/LibriTTS_VCTK.orig.yaml
    # model_config: config/model/base.yaml
    train_config: &train_config
      - config/train/LibriTTS.yaml
      - config/train/base.yaml
    # preprocess_config: *preprocess_config
    # train_config: *train_config
    algorithm_config:
      type: meta  # meta/baseline/imaml, get_system

      _phn_emb_config:
        embedding: &embedding
          type: embedding
          refresh: False

      adapt:
        type: spk # spk/lang
        #class: MAML # MAML/iMAML, not used
        speaker_emb: table # shared/table/encoder
        phoneme_emb: *embedding  # *embedding/*codebook
        modules:
          - speaker_emb
          - variance_adaptor
          - decoder
          - mel_linear
          - postnet

        task: &task
          ways: 1
          shots: 5
          queries: 5
          lr: &lr 0.001

        learner_config:
          class_path: lightning.algorithms.MAML.MAML
          init_args:
            lr: *lr

        train:
          << : *task
          steps: 5
          meta_batch_size: 8

        test:
          << : *task
          queries: 1
          steps: 100 # max adaptation steps for testing
          saving_steps: [5, 10, 20, 50, 100, 200, 400, 600, 800, 1000]
          avg_train_spk_emb: False
          1-shot: False
    stage: train

trainer:
  ### Train settings
  max_steps: 100000
  # gradient_clip_val: 1.0
  # accumulate_grad_batches: 1

  ### Logging and saving settings
  enable_checkpointing: True
  default_root_dir: output/pruned_maml
  log_every_n_steps: 100
  check_val_every_n_epoch: 10

  ### GPU-related settings
  accelerator: gpu
  # devices: [0]
  # strategy: ddp
  # auto_select_gpus: True

  ### Callbacks
  # callbacks:
  #   - class_path: pytorch_lightning.callbacks.DeviceStatsMonitor
  #     init_args: {}
  #   - class_path: pytorch_lightning.callbacks.ModelCheckpoint
  #     init_args:
  #       save_top_k: -1
  #       every_n_epochs: 100
  #   #     save_weights_only: True
  #   - class_path: pytorch_lightning.callbacks.LearningRateMonitor
  #     init_args:
  #       log_momentum: True

  ### Malicious settings
  profiler: simple
  detect_anomaly: True

  ### Depreciated settings
  # deterministic: True
