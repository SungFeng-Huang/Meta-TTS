# log_dir: output/log/pretrain_LibriTTS.2
# result_dir: output/result/dev.2

seed_everything: 42
model:
  class_path: projects.prune.systems.prune_accent.PruneAccentSystem
  init_args:
    ckpt_path: output/ckpt/LibriTTS/meta-tts-accent-dev/1d34914e07244f87ad10c580ab85656c/checkpoints/epoch=39-step=39999.ckpt
    qry_patience: 200

    preprocess_config: &preprocess_config config/preprocess/LibriTTS_VCTK.yaml
    model_config: config/model/base.yaml
    algorithm_config: config/algorithm/pretrain/pretrain_LibriTTS.2.yaml

    accent_ckpt: "output/xvec/lightning_logs/version_88/checkpoints/epoch=199-step=164200.ckpt"
    speaker_ckpt: "output/xvec/lightning_logs/version_99/checkpoints/epoch=49-step=377950.ckpt"

data:
  class_path: lightning.datamodules.prune_accent_datamodule.PruneAccentDataModule
  init_args:
    steps_per_epoch: 100
    k: 5
    q: 8
    key: p225
    preprocess_config: *preprocess_config
    train_config: &train_config
      - config/train/LibriTTS.yaml
      - config/train/base.yaml
    # batch_size: 80

trainer:
  ### Train settings
  max_epochs: 10
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1

  ### Logging and saving settings
  enable_checkpointing: False
  default_root_dir: output/prune_lottery_tickets/p225

  ### GPU-related settings
  accelerator: gpu
  devices: [0]
  strategy: ddp
  auto_select_gpus: True

  ### Callbacks
  callbacks:
    # - class_path: pytorch_lightning.callbacks.ModelPruning
    #   init_args:
    #     pruning_fn: l1_unstructured
    #     parameters_to_prune: null
    #     use_global_unstructured: True
    #     amount: *amount # no bit use. still need to override on_fit_start
    #     apply_pruning: True
    #     # make_pruning_permanent: False
    #     use_lottery_ticket_hypothesis: True
    #     resample_parameters: False
    #     verbose: 2
    #     prune_on_train_epoch_end: True
    # - class_path: pytorch_lightning.callbacks.EarlyStopping
    #   init_args:
    #     monitor: accent_acc/val
    #     mode: max
    #     verbose: 1
    - class_path: lightning.callbacks.saver.prune_accent.PruneAccentSaver
      init_args:
        preprocess_config: *preprocess_config
    - class_path: pytorch_lightning.callbacks.DeviceStatsMonitor
      init_args: {}
    # - class_path: pytorch_lightning.callbacks.ModelCheckpoint
    #   init_args:
    #     filename: 'epoch={epoch}-step={step}-vacc={accent_acc-val/val_end:.2f}-vprob={accent_prob-val/val_end:.2f}'
    #     monitor: accent_acc-val/val_end
    #     mode: max
    #     save_top_k: -1
    #     auto_insert_metric_name: False
    - class_path: pytorch_lightning.callbacks.LearningRateMonitor
      init_args:
        log_momentum: True

  ### Malicious settings
  profiler: simple

  ### Depreciated settings
  # deterministic: True

model_pruning_callback:
  amount: 0.1
